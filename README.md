
# ğŸ§  Attention-YOLOv8

This project focuses on enhancing the YOLOv8 architecture by integrating various attention mechanisms to improve detection performance.

Traditional object detection architectures may benefit from embedded attention modules that help the network focus on more informative spatial and channel-wise features. This repository experiments with several popular attention modules within the YOLOv8 model.

---

## âœ… Attention Modules Checklist

- âœ… **SKAttention** (implemented and tested)
- âœ… **CBAM** (implemented and tested)
- âœ… **PSA** (implemented and tested)
- âœ… **SimAM** (implemented and tested)
- â³ **GAM** (planned)
- â³ **SE** (planned)

---

## âš™ï¸ Example Configuration Block with SKAttention

```yaml
- [-1, 1, SKAttention, [1024, [3, 5, 7], 16]]
```

- `1024` â€” Number of input/output channels  
- `[3, 5, 7]` â€” Multi-scale kernel sizes for spatial attention  
- `16` â€” Channel reduction ratio in the attention block  

---

## âš™ï¸ Example Configuration Block with PSA

```yaml
- [-1, 1, PSAPlug, [1024,4]]
```

- `1024` â€” Number of input/output channels  
- `4` â€” Channel reduction ratio in the attention block

---

## âš™ï¸ Example Configuration Block with CBAM

```yaml
- [-1, 1, CBAM, [256,3]]
```

- `256` â€” Number of input/output channels  
- `3` â€” Size of the convolutional kernel for spatial attention

---

## ğŸ§ª Sample Training Script

```python
from ultralytics import YOLO

model = YOLO("yolov8s-sk.yaml")
model.train(
    data="your-dataset.yaml",
    epochs=100,
    imgsz=640,
    batch=32,
    name="yolov8s-sk"
)
```

---

## ğŸ“Œ Future Plans

- Benchmark each attention module on the same dataset.
- Analyze the contribution of each attention block during training.
- Add performance comparison table (mAP, precision, recall) across modules.
